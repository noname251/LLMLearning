{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头注意力\n",
    "# 创建模型结构\n",
    "# 问题1：前面这一这段代码的含义是什么，python相关\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, is_mask = False):\n",
    "        # 注意这里super要传入自己和self，不能只传入自己，你要多学一些python的基础知识了\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # 初始化超参数\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        # 创建qkv的三个线性映射层，用一个矩阵和qkv相乘，让qkv变得可学习\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        # 问题2：这个combine的作用是什么，矩阵经过怎样的运算\n",
    "        self.combine = nn.Linear(d_model,d_model)\n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        batch, time, dimension = q.shape\n",
    "        # 得到每个头的dimension\n",
    "        n_d = d_model // n_head\n",
    "        # 让qkv进入线性层\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        # 拆分qkv\n",
    "        # 问题3:为什么需要permute这个东西： \n",
    "        # 问题4:这个转置函数怎么用的，结果是什么\n",
    "        # 问题5:这两个四维矩阵是怎么乘起来的\n",
    "        # 为了更好的并行计算，变换之后，qkv矩阵形状变成(batch, n_head, time, n_d)，之后k要再次变换成(batch, n_head, n_d, time)\n",
    "        # 最后q @ k结果的shape是(batch, n_head, time, time)两个四维矩阵相乘，前面两个维度都不参与运算。\n",
    "        q = q.view(batch, time, n_head, n_d).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, time, n_head, n_d).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, time, n_head, n_d).permute(0, 2, 1, 3)\n",
    "        # 都是交换维度，只是交换维度的数量区别\n",
    "        # 计算注意力结果 q*k转置/n_d\n",
    "        # score的shape为(batch, n_head, time, time)\n",
    "        score = q @ k.transpose(2,3) / math.sqrt(n_d)\n",
    "        # 生成mask，一个左下角为1的下三角矩阵\n",
    "        if is_mask:\n",
    "            mask = torch.tril(torch.ones(time, time, dtype=bool))\n",
    "        # 让mask掩盖score中不应该被注意的数值,全部变成负无穷\n",
    "        # 问题6：这个masked_fill函数是哪里的，矩阵自带的吗，怎么用， pytorch张量自带的方法，用于用特定值填充掩码位置。\n",
    "        # 为什么需要combine\n",
    "        # 在多头注意力机制中，将多个头的输出重新组合成一个张量后，还需要通过一个线性变换来生成最终的输出。这是因为每个头的输出仅仅代表该头的注意力结果，最后通过线性层来融合各个头的信息，生成最终的表示。\n",
    "        # 1. 线性变换：尽管 permute 和 view 将多个头的输出重新组合成了一个张量，但是每个头的输出还需要进一步线性变换，以便在融合信息的同时对其进行适当的缩放和变换。\n",
    "        # 2. 参数化：combine 层通过线性层实现参数化，使得整个模型能够学习如何将多个头的注意力结果组合成最终的输出表示。这是模型学习的关键部分。\n",
    "        # 3. 增加模型表达能力：线性层通过增加可学习参数，提升了模型的表达能力，使其能够更好地捕捉复杂的模式和关系。\n",
    "            score = score.masked_fill(mask == 0, float('-inf'))\n",
    "        # 经过softmax然后@v, 点乘之后最后两个维度相乘，又变成(batch, n_head, time, n_d)了\n",
    "        score = self.softmax(score) @ v\n",
    "\n",
    "        # 把score的形状变换回来\n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)\n",
    "\n",
    "        # combine score\n",
    "        # 问题7：这个combine的作用是什么\n",
    "        output = self.combine(score)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__(vocab_size, d_model, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoder(nn.Module):\n",
    "    # 输入为词向量长度，序列最大长度，设备\n",
    "    def __init__(self, d_model, maxlen):\n",
    "        # 初始化编码\n",
    "        self.encoding = torch.zeros(maxlen, d_model)\n",
    "        self.encoding.requires_grad = False\n",
    "        # 初始化位置\n",
    "        pos = torch.arange(0, maxlen)\n",
    "        # 这里为什么要变成二维的\n",
    "        pos = pos.float().unsqueeze(1)\n",
    "        _2i = torch.arange(0, d_model, 2)\n",
    "        # 广播机制，一个(5,1)的数组/（2）的数组，结果为（5，2）的数组\n",
    "        # 生成位置编码\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        len = x.shape[1]\n",
    "        return self.encoding[:len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Pameter(torch.ones(d_model))\n",
    "        self.beta = nn.Pameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x最后一个维度的均值和方差，也就是d_model\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased = False, keepdim=True)\n",
    "        out = x - mean / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size, drop_out = 0.1):\n",
    "        super().__init__()\n",
    "        fc1 = nn.Linear(d_model, hidden_size)\n",
    "        fc1 = nn.Linear(hidden_size, d_model)\n",
    "        drop_out = nn.Dropout(drop_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 第一层 -> relu -> drop_out -> 第二层\n",
    "        x = fc1(x)\n",
    "        x = drop_out(F.relu(x))\n",
    "        return fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, maxlen, drop_out = 0.1):\n",
    "        super().__init__()\n",
    "        # 词嵌入层\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.position_encoder = PositionEncoder(d_model, maxlen)\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 词嵌入层 -> 位置编码层 -> drop_out\n",
    "        tok_embedding = self.token_embedding(x)\n",
    "        pos_encode = self.position_encoder(x)\n",
    "        return drop_out(tok_embedding + pos_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, maxlen, hidden_size, drop_out = 0.1):\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, hidden_size, is_mask=False)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(drop_out)\n",
    "        \n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.drop2 = nn.Dropout(drop_out)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_model, hidden_size, drop_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 多头注意力 -> 残差连接 -> 层归 -> 位置前馈 -> 残差连接 -> 层归\n",
    "        _x = x\n",
    "        x = self.multi_head_attention(x)\n",
    "        x = self.layer_norm1(self.drop1(x))\n",
    "        x = _x + x\n",
    "\n",
    "        _x = x\n",
    "        x = self.position_wise_feed_forward(x)\n",
    "        x = self.layer_norm2(self.drop2(x))\n",
    "        return x + _x \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "torch.Size([2])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [1.0000, 0.5000],\n",
      "        [2.0000, 1.0000],\n",
      "        [3.0000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4)\n",
    "x = x.float().unsqueeze(1)\n",
    "print(x.shape)\n",
    "y = torch.arange(1, 3)\n",
    "print(y.shape)\n",
    "print(x / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 2, 3)\n",
    "mean = x.mean(-1, keepdim=True)\n",
    "print(x.shape)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
