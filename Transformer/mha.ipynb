{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cc49d3",
   "metadata": {},
   "source": [
    "# 第一个版本的MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed93f910-ef27-4461-8712-2f007754886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf4ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入模型超参数\n",
    "batch_model = 2\n",
    "time_model = 3\n",
    "d_model = 4\n",
    "n_head = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ddd811-769b-4f74-8d2c-e1cb5131a2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6769,  0.6549, -0.0496, -1.5999],\n",
       "         [-1.3586, -0.6275, -0.9502, -0.3471],\n",
       "         [-0.6865,  0.1135,  0.9351,  0.1410]],\n",
       "\n",
       "        [[-0.1647,  0.2453,  0.1300, -0.8140],\n",
       "         [ 0.1070,  1.3536, -1.1907,  1.2817],\n",
       "         [ 0.8633, -0.0731,  0.0484, -1.6660]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机创建输入, qkv\n",
    "X = torch.randn(batch_model, time_model, d_model)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8659edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 1])\n",
      "------------\n",
      "torch.Size([2, 4, 1, 3])\n",
      "torch.Size([2, 4, 1, 3])\n",
      "tensor([[[ 0.2031, -0.3362, -0.5117, -0.0695],\n",
      "         [ 0.1738, -0.2666, -0.5891, -0.0890],\n",
      "         [ 0.2122, -0.3611, -0.5845, -0.2986]],\n",
      "\n",
      "        [[ 0.1462, -0.3619, -0.4441,  0.1060],\n",
      "         [-0.2384, -0.6000, -0.3893,  0.7503],\n",
      "         [-0.5753, -0.6997, -0.6862,  0.8505]]], grad_fn=<ViewBackward0>) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 创建模型结构\n",
    "# 问题1：前面这一这段代码的含义是什么，python相关\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        # 注意这里super要传入自己和self，不能只传入自己，你要多学一些python的基础知识了\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # 初始化超参数\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        # 创建qkv的三个线性映射层，用一个矩阵和qkv相乘，让qkv变得可学习\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        # 问题2：这个combine的作用是什么，矩阵经过怎样的运算\n",
    "        self.combine = nn.Linear(d_model,d_model)\n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        batch, time, dimension = q.shape\n",
    "        # 得到每个头的dimension\n",
    "        n_d = d_model // n_head\n",
    "        # 让qkv进入线性层\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        # 拆分qkv\n",
    "        # 问题3:为什么需要permute这个东西： \n",
    "        # 问题4:这个转置函数怎么用的，结果是什么\n",
    "        # 问题5:这两个四维矩阵是怎么乘起来的\n",
    "        # 为了更好的并行计算，变换之后，qkv矩阵形状变成(batch, n_head, time, n_d)，之后k要再次变换成(batch, time, m_head, n_d)\n",
    "        # 最后q @ k结果的shape是(batch, n_head, time, time)两个四维矩阵相乘，前面两个维度都不参与运算。\n",
    "        q = q.view(batch, time, n_head, n_d).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, time, n_head, n_d).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, time, n_head, n_d).permute(0, 2, 1, 3)\n",
    "        # 都是交换维度，只是交换维度的数量区别\n",
    "        print(k.shape)\n",
    "        print('------------')\n",
    "        print(k.permute(0, 1, 3, 2).shape)\n",
    "        print(k.transpose(2, 3).shape)\n",
    "        # 计算注意力结果 q*k转置/n_d\n",
    "        # score的shape为(batch, n_head, time, time)\n",
    "        score = q @ k.transpose(2,3) / math.sqrt(n_d)\n",
    "        # 生成mask，一个左下角为1的下三角矩阵\n",
    "        mask = torch.tril(torch.ones(time, time, dtype=bool))\n",
    "        # 让mask掩盖score中不应该被注意的数值,全部变成负无穷\n",
    "        # 问题6：这个masked_fill函数是哪里的，矩阵自带的吗，怎么用， pytorch张量自带的方法，用于用特定值填充掩码位置。\n",
    "        # 为什么需要combine\n",
    "        # 在多头注意力机制中，将多个头的输出重新组合成一个张量后，还需要通过一个线性变换来生成最终的输出。这是因为每个头的输出仅仅代表该头的注意力结果，最后通过线性层来融合各个头的信息，生成最终的表示。\n",
    "        # 1. 线性变换：尽管 permute 和 view 将多个头的输出重新组合成了一个张量，但是每个头的输出还需要进一步线性变换，以便在融合信息的同时对其进行适当的缩放和变换。\n",
    "        # 2. 参数化：combine 层通过线性层实现参数化，使得整个模型能够学习如何将多个头的注意力结果组合成最终的输出表示。这是模型学习的关键部分。\n",
    "        # 3. 增加模型表达能力：线性层通过增加可学习参数，提升了模型的表达能力，使其能够更好地捕捉复杂的模式和关系。\n",
    "        score = score.masked_fill(mask == 0, float('-inf'))\n",
    "        # 经过softmax然后@v, 点乘之后最后两个维度相乘，又变成(batch, n_head, time, n_d)了\n",
    "        score = self.softmax(score) @ v\n",
    "\n",
    "        # 把score的形状变换回来\n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)\n",
    "\n",
    "        # combine score\n",
    "        # 问题7：这个combine的作用是什么\n",
    "        output = self.combine(score)\n",
    "        return output\n",
    "attention = MultiHeadAttention(d_model, n_head)\n",
    "output = attention(X, X, X)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cd6ed",
   "metadata": {},
   "source": [
    "# 第二个版本的MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38c2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, List\n",
    "from labml import tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "854efff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性层+拆分多头\n",
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, d_k: int,  bias: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, n_head * d_k, bias=bias)\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear(x)\n",
    "        # 拆分最后一层\n",
    "        head = x.shape[:-1]\n",
    "        x = x.view(*head, self.n_head, self.d_k)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d0f2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qkv->线性层->qkv拆分->注意力计算、缩放->mask->softmax->drop->注意力分数计算->线性层\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, drop_out: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        # 计算头数量\n",
    "        self.d_k = d_model // n_head\n",
    "        self.n_head = n_head\n",
    "        # 线性层+拆分多头\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, n_head, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, n_head, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, n_head, self.d_k, bias=bias)\n",
    "        self.scores = 0\n",
    "        # 缩放因子\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "        # softmax, 对时间维度做softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # 线性层\n",
    "        self.output = nn.Linear(d_model, d_model, bias=bias)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    \n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "\n",
    "    def prepareForMask(self, mask: torch.Tensor, query: torch.Tensor, key: torch.Tensor):\n",
    "        # mask 的形状为[seq_len_q, seq_len_k, batch_size] ，其中第一维是查询维度。如果查询维度等于1，则会进行广播。\n",
    "        assert mask[0] == 1 or mask[0] == query.shape[0]\n",
    "        assert mask[1] == key.shape[0]\n",
    "        assert mask[2] == 1 or mask[2] == query.shape[1]\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        return mask\n",
    "\n",
    "    # 问题： 这个*是什么意思\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        # 计算注意力权重\n",
    "        # 变换之后query的形状为(seq_len, batch_size, n_head, d_k)\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "        # 准备掩码\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        \n",
    "        scores = self.get_scores(query, key)\n",
    "        # 缩放\n",
    "        scores = scores * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = self.softmax(scores)\n",
    "        # 调试保存注意力信息\n",
    "        # tracker.debug('attn', attn)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # 计算注意力\n",
    "        # 问题： 爱因斯坦求和的过程是怎样的\n",
    "        x = torch.einsum('ijbh,jbhd->ibhd', attn, value)\n",
    "        # 问题： 这句话的作用是什么\n",
    "        self.attn = attn.detach()\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        # 最后加入线性层\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d562b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "seq_len = 1\n",
    "batch_size = 2\n",
    "d_model = 6\n",
    "n_head = 2\n",
    "X = torch.randn(seq_len, batch_size, d_model)\n",
    "\n",
    "multiHeadAttention = MultiHeadAttention(d_model, n_head)\n",
    "output = multiHeadAttention(X, X, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
